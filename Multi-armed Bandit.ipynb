{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Multi-armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu18/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 4 arms of the bandit : [1. 0. 0. 0.]\n",
      "Running reward for the 4 arms of the bandit : [-3. -3.  3. 12.]\n",
      "Running reward for the 4 arms of the bandit : [-3. -4.  2. 24.]\n",
      "Running reward for the 4 arms of the bandit : [-8. -4.  7. 36.]\n",
      "Running reward for the 4 arms of the bandit : [-11.   0.  10.  52.]\n",
      "Running reward for the 4 arms of the bandit : [-10.   1.   7.  67.]\n",
      "Running reward for the 4 arms of the bandit : [-15.  -1.  12.  75.]\n",
      "Running reward for the 4 arms of the bandit : [-20.   2.  17.  84.]\n",
      "Running reward for the 4 arms of the bandit : [-18.   3.  26. 100.]\n",
      "Running reward for the 4 arms of the bandit : [-20.   7.  32. 110.]\n",
      "Running reward for the 4 arms of the bandit : [-22.   8.  36. 127.]\n",
      "Running reward for the 4 arms of the bandit : [-24.  13.  45. 143.]\n",
      "Running reward for the 4 arms of the bandit : [-25.  14.  51. 163.]\n",
      "Running reward for the 4 arms of the bandit : [-26.  11.  55. 179.]\n",
      "Running reward for the 4 arms of the bandit : [-23.   9.  62. 193.]\n",
      "Running reward for the 4 arms of the bandit : [-26.  11.  67. 207.]\n",
      "Running reward for the 4 arms of the bandit : [-26.  10.  77. 220.]\n",
      "Running reward for the 4 arms of the bandit : [-21.  13.  80. 237.]\n",
      "Running reward for the 4 arms of the bandit : [-20.   8.  83. 256.]\n",
      "Running reward for the 4 arms of the bandit : [-24.   8.  82. 271.]\n",
      "\n",
      "The agent thinks arm 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "# create bandit index\n",
    "# in current situation, 4th bandit will create max reward\n",
    "bandit_arms = [0.2, 0, -0.2, -2]\n",
    "num_arms = len(bandit_arms)\n",
    "\n",
    "def pullBandit(bandit):\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else :\n",
    "        return -1\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# network and feedforward\n",
    "weights = tf.Variable(tf.ones([num_arms]))  \n",
    "output = tf.nn.softmax(weights)  # weights of each arm\n",
    "\n",
    "# training\n",
    "reward_holder = tf.placeholder(shape = [1], dtype = tf.float32)\n",
    "action_holder = tf.placeholder(shape = [1], dtype = tf.int32)\n",
    "\n",
    "responsible_output = tf.slice(output, action_holder, [1])  # probability of selected arm\n",
    "loss = -(tf.log(responsible_output) * reward_holder)  # loss = -log(policy) * quantity of the action works better than any baseline(in this case reward)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 1e-3)\n",
    "update = optimizer.minimize(loss)\n",
    "\n",
    "# set the number of episode\n",
    "total_episodes = 1000\n",
    "\n",
    "# initialize reward\n",
    "total_reward = np.zeros(num_arms)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# launching tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        actions = sess.run(output)\n",
    "        a = np.random.choice(actions, p = actions)\n",
    "        action = np.argmax(actions == a)\n",
    "        \n",
    "        reward = pullBandit(bandit_arms[action])\n",
    "        \n",
    "        _,resp,ww = sess.run([update, responsible_output, weights], feed_dict = {reward_holder : [reward], action_holder : [action]})\n",
    "        \n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print(\"Running reward for the \" + str(num_arms) + \" arms of the bandit : \" + str(total_reward))\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "print(\"\\nThe agent thinks arm \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandit_arms)):\n",
    "    print(\"...and it was right!\")\n",
    "else:\n",
    "    print(\"...and it was wrong!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0710760550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0710760550>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0710760550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0710760550>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "Mean reward for each of the 3bandits : [-0.25  0.    0.  ]\n",
      "Mean reward for each of the 3bandits : [33.   40.   34.25]\n",
      "Mean reward for each of the 3bandits : [73.   77.75 70.  ]\n",
      "Mean reward for each of the 3bandits : [109.5  117.   108.25]\n",
      "Mean reward for each of the 3bandits : [148.   159.   142.25]\n",
      "Mean reward for each of the 3bandits : [186.5  197.5  175.75]\n",
      "Mean reward for each of the 3bandits : [230.   236.   206.75]\n",
      "Mean reward for each of the 3bandits : [266.75 275.5  241.5 ]\n",
      "Mean reward for each of the 3bandits : [309.25 309.75 279.25]\n",
      "Mean reward for each of the 3bandits : [345.   349.75 315.5 ]\n",
      "Mean reward for each of the 3bandits : [385.5  387.   349.25]\n",
      "Mean reward for each of the 3bandits : [424.   425.25 381.  ]\n",
      "Mean reward for each of the 3bandits : [463.75 463.75 413.75]\n",
      "Mean reward for each of the 3bandits : [503.5  503.25 445.5 ]\n",
      "Mean reward for each of the 3bandits : [543.   540.5  481.75]\n",
      "Mean reward for each of the 3bandits : [580.25 574.5  522.  ]\n",
      "Mean reward for each of the 3bandits : [615.5  611.25 563.  ]\n",
      "Mean reward for each of the 3bandits : [653.75 651.75 596.25]\n",
      "Mean reward for each of the 3bandits : [690.75 686.75 629.75]\n",
      "Mean reward for each of the 3bandits : [730.5  723.75 663.5 ]\n",
      "The agent thinks action 4 for bandit 1 is the most promising...\n",
      "...and it was right!\n",
      "The agent thinks action 2 for bandit 2 is the most promising...\n",
      "...and it was right!\n",
      "The agent thinks action 1 for bandit 3 is the most promising...\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "# context bandit 3 bandits composed with 4 arms\n",
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.bandits = np.array([[0.2, 0, -0.0, -5], [0.1, -5, 1, 0.25], [-5, 5, 5, 5]])  # index of arms. 4th, 2nd and 1st arms are optimum resp.\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        #return random state for each episode\n",
    "        self.state = np.random.randint(0, len(self.bandits))  # in this case, the state is which bandit I chose\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self, action):\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "\n",
    "# policy based agent\n",
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size):  # s_size = state size, a_size = action size\n",
    "        # feedforward of network\n",
    "        self.state_in = tf.placeholder(shape = [1], dtype = tf.int32)\n",
    "        state_in_OH = slim.one_hot_encoding(self.state_in, s_size)\n",
    "        output = slim.fully_connected(state_in_OH, a_size, biases_initializer = None,\n",
    "                                      activation_fn = tf.nn.sigmoid, \n",
    "                                      weights_initializer = tf.ones_initializer())\n",
    "        self.output = tf.reshape(output, [-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        # define variables while training\n",
    "        # to calculate reward, feed reward and action to the network and use them to update network\n",
    "        self.reward_holder = tf.placeholder(shape = [1], dtype = tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape = [1], dtype = tf.int32)\n",
    "        self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
    "        self.loss = -(tf.log(self.responsible_weight) * self.reward_holder)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr)\n",
    "        self.update = optimizer.minimize(self.loss)\n",
    "        \n",
    "# training agent\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# load bandit\n",
    "cBandit = contextual_bandit()\n",
    "# load agent\n",
    "myAgent = agent(lr = 0.001, s_size = cBandit.num_bandits, a_size = cBandit.num_actions)\n",
    "\n",
    "weights = tf.trainable_variables()[0]\n",
    "total_episodes = 10000\n",
    "total_reward = np.zeros([cBandit.num_bandits, cBandit.num_actions])\n",
    "# probability of greedy\n",
    "e = 0.1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# launching tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        s = cBandit.getBandit()\n",
    "        # random action or one action\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(cBandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action, feed_dict = {myAgent.state_in:[s]})\n",
    "        \n",
    "        reward = cBandit.pullArm(action)\n",
    "        \n",
    "        feed_dict = {myAgent.reward_holder : [reward], myAgent.action_holder : [action], myAgent.state_in : [s]}\n",
    "        _, ww = sess.run([myAgent.update, weights], feed_dict = feed_dict)\n",
    "        \n",
    "        total_reward[s, action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print(\"Mean reward for each of the \" + str(cBandit.num_bandits) + \n",
    "                  \"bandits : \" + str(np.mean(total_reward, axis = 1)))\n",
    "        i += 1\n",
    "        \n",
    "for a in range(cBandit.num_bandits):\n",
    "    print(\"The agent thinks action \" + str(np.argmax(ww[a]) + 1) + \n",
    "          \" for bandit \" + str(a+1) + \" is the most promising...\")\n",
    "    if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]):\n",
    "        print(\"...and it was right!\")\n",
    "    else:\n",
    "        print(\"...and it was wrong!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
