{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Policy Gradient Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3247f9cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3247f9cf90>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3247f9cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3247f9cf90>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3247ee14d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3247ee14d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3247ee14d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3247ee14d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "36.0\n",
      "22.43\n",
      "23.75\n",
      "24.97\n",
      "26.64\n",
      "26.51\n",
      "28.56\n",
      "24.72\n",
      "26.15\n",
      "27.15\n",
      "26.11\n",
      "25.66\n",
      "27.04\n",
      "25.21\n",
      "26.72\n",
      "24.56\n",
      "24.17\n",
      "27.38\n",
      "26.87\n",
      "27.4\n",
      "23.63\n",
      "25.55\n",
      "22.35\n",
      "22.13\n",
      "24.6\n",
      "24.45\n",
      "21.34\n",
      "21.87\n",
      "23.15\n",
      "21.94\n",
      "21.78\n",
      "22.85\n",
      "23.07\n",
      "26.28\n",
      "23.76\n",
      "22.75\n",
      "23.96\n",
      "22.32\n",
      "24.49\n",
      "24.49\n",
      "23.47\n",
      "24.98\n",
      "25.7\n",
      "23.45\n",
      "25.13\n",
      "24.84\n",
      "27.75\n",
      "25.69\n",
      "25.78\n",
      "25.29\n"
     ]
    }
   ],
   "source": [
    "# import environment from openAI Gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# reward function and agent\n",
    "gamma = 0.9\n",
    "\n",
    "def discount_rewards(r):\n",
    "    # compute discounted rewards \n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "        \n",
    "    return discounted_r\n",
    "\n",
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size, h_size):\n",
    "        # feedforward of network\n",
    "        self.state_in = tf.placeholder(shape = [None, s_size], dtype = tf.float32)\n",
    "        hidden = slim.fully_connected(self.state_in, h_size, biases_initializer = None, activation_fn = tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden, a_size, biases_initializer = None, activation_fn = tf.nn.softmax)\n",
    "        self.chosen_action = tf.argmax(self.output, 1)\n",
    "        \n",
    "        # training process\n",
    "        # to calculate cost, feed reward and action to network and use it to update\n",
    "        self.reward_holder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "        \n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "        \n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs) * self.reward_holder)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx, var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32, name = str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "            \n",
    "        self.gradients = tf.gradients(self.loss, tvars)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, tvars))\n",
    "        \n",
    "\n",
    "# training agent\n",
    "tf.reset_default_graph()\n",
    "\n",
    "myAgent = agent(lr = 1e-2, s_size = 4, a_size = 2, h_size = 8)\n",
    "total_episodes = 5000\n",
    "max_ep = 999\n",
    "update_frequency = 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    total_reward = []\n",
    "    total_lenght = []\n",
    "    \n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "        \n",
    "    while i < total_episodes:\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_ep):\n",
    "            # from output of network, choose action with probability\n",
    "            a_dist = sess.run(myAgent.output, feed_dict = {myAgent.state_in:[s]})\n",
    "            a = np.random.choice(a_dist[0], p = a_dist[0])\n",
    "            a = np.argmax(a_dist == a)\n",
    "            \n",
    "            # reward about action\n",
    "            s1, r, d, _ = env.step(a)\n",
    "            ep_history.append([s, a, r, s1])\n",
    "            s = s1\n",
    "            running_reward += r\n",
    "            if d == True:\n",
    "                # update network\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 2] = discount_rewards(ep_history[:, 2])\n",
    "                feed_dict = {myAgent.reward_holder:ep_history[:, 2],\n",
    "                            myAgent.action_holder:ep_history[:, 1],\n",
    "                            myAgent.state_in:np.vstack(ep_history[:,0])}\n",
    "                grads = sess.run(myAgent.gradients, feed_dict = feed_dict)\n",
    "                for idx, grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "                    \n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    feed_dict = dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict = feed_dict)\n",
    "                    for ix, grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "                        \n",
    "                total_reward.append(running_reward)\n",
    "                total_lenght.append(j)\n",
    "                break\n",
    "                \n",
    "                \n",
    "        # update total reward\n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(total_reward[-100:]))\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([ 0.01245611, -0.01089109, -0.01127786,  0.04325728]) 1\n",
      "  6.12579511 array([ 0.01223829,  0.18439075, -0.01041272, -0.25296247])]\n",
      " [array([ 0.01223829,  0.18439075, -0.01041272, -0.25296247]) 1\n",
      "  5.6953279000000006\n",
      "  array([ 0.0159261 ,  0.37965982, -0.01547197, -0.54891149])]\n",
      " [array([ 0.0159261 ,  0.37965982, -0.01547197, -0.54891149]) 1 5.217031\n",
      "  array([ 0.0235193 ,  0.57499566, -0.0264502 , -0.84642875])]\n",
      " [array([ 0.0235193 ,  0.57499566, -0.0264502 , -0.84642875]) 1 4.68559\n",
      "  array([ 0.03501921,  0.77046828, -0.04337877, -1.14731057])]\n",
      " [array([ 0.03501921,  0.77046828, -0.04337877, -1.14731057]) 1 4.0951\n",
      "  array([ 0.05042858,  0.96612895, -0.06632498, -1.45327485])]\n",
      " [array([ 0.05042858,  0.96612895, -0.06632498, -1.45327485]) 1 3.439\n",
      "  array([ 0.06975116,  1.1619998 , -0.09539048, -1.76592038])]\n",
      " [array([ 0.06975116,  1.1619998 , -0.09539048, -1.76592038]) 1 2.71\n",
      "  array([ 0.09299115,  1.35806158, -0.13070889, -2.08667834])]\n",
      " [array([ 0.09299115,  1.35806158, -0.13070889, -2.08667834]) 1 1.9\n",
      "  array([ 0.12015238,  1.55423929, -0.17244245, -2.41675381])]\n",
      " [array([ 0.12015238,  1.55423929, -0.17244245, -2.41675381]) 1 1.0\n",
      "  array([ 0.15123717,  1.75038526, -0.22077753, -2.75705627])]]\n",
      "<TimeLimit<CartPoleEnv<CartPole-v0>>>\n",
      "[26.390107090000008 22.515902200000006 18.689527000000005\n",
      " 14.969440000000002 11.4265 8.146 5.23 2.8 1.0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fa1d39910e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "print(ep_history)\n",
    "print(env)\n",
    "print(discount_rewards(ep_history[:, 2]))\n",
    "a = sess.run(myAgent.indexes, feed_dict = feed_dict)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
